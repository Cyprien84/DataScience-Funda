{"cells":[{"cell_type":"markdown","source":["# Varables :\n<br>Here's a brief version of what you'll find in the data description file.:\n<br> **SalePrice** </br> : the property's sale price in dollars. This is the target variable that you're trying to predict.:\n<br>**MSZoning**: Identifies the general zoning classification of the sale.\n<br>**GrLivArea**: Above grade (ground) living area square feet.\n<br>**Utilities**: Type of utilities available.\n<br>**OverallQual**: Rates the overall material and finish of the house.\n<br>**YearBuilt**: Original construction date.\n<br>**TotalBsmtSF**: Total square feet of basement area.\n<br>**Heating**: Type of heating.\n<br>**HeatingQC**: Heating quality and condition.\n<br>**CentralAir**: Central air conditioning.\n<br>**FullBath**: Full bathrooms above grade.\n<br>**BedroomAbvGr**: Bedrooms above grade (does NOT include basement bedrooms)\n<br>**KitchenAbvGr**: Kitchens above grade\n<br>**GarageCars**: Size of garage in car capacity.\n<br>**PoolArea**: Pool area in square feet."],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\n%matplotlib inline\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#used in correlation\nimport scipy.stats as ss\nimport numpy as np\nimport itertools\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import GridSearchCV\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"markdown","source":["## 1- Import data"],"metadata":{}},{"cell_type":"markdown","source":["Load the csv file with our data into a pandas Dataframe"],"metadata":{}},{"cell_type":"code","source":["data = pd.read_csv('data/train_custom.csv')\nprint(\"Number of houses in our Dataset : \", data.shape[0])\nprint(\"Number of features : \", data.shape[1]-1)#the -1 is for not counting the Target SalePrice"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">FileNotFoundError</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1892949054938736&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>data <span class=\"ansi-blue-fg\">=</span> pd<span class=\"ansi-blue-fg\">.</span>read_csv<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;data/train_custom.csv&#39;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> print<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Number of houses in our Dataset : &#34;</span><span class=\"ansi-blue-fg\">,</span> data<span class=\"ansi-blue-fg\">.</span>shape<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> print<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Number of features : &#34;</span><span class=\"ansi-blue-fg\">,</span> data<span class=\"ansi-blue-fg\">.</span>shape<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">-</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">#the -1 is for not counting the Target SalePrice</span>\n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/pandas/io/parsers.py</span> in <span class=\"ansi-cyan-fg\">parser_f</span><span class=\"ansi-blue-fg\">(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    700</span>                     skip_blank_lines=skip_blank_lines)\n<span class=\"ansi-green-intense-fg ansi-bold\">    701</span> \n<span class=\"ansi-green-fg\">--&gt; 702</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> _read<span class=\"ansi-blue-fg\">(</span>filepath_or_buffer<span class=\"ansi-blue-fg\">,</span> kwds<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    703</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    704</span>     parser_f<span class=\"ansi-blue-fg\">.</span>__name__ <span class=\"ansi-blue-fg\">=</span> name\n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/pandas/io/parsers.py</span> in <span class=\"ansi-cyan-fg\">_read</span><span class=\"ansi-blue-fg\">(filepath_or_buffer, kwds)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    427</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    428</span>     <span class=\"ansi-red-fg\"># Create the parser.</span>\n<span class=\"ansi-green-fg\">--&gt; 429</span><span class=\"ansi-red-fg\">     </span>parser <span class=\"ansi-blue-fg\">=</span> TextFileReader<span class=\"ansi-blue-fg\">(</span>filepath_or_buffer<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kwds<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    430</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    431</span>     <span class=\"ansi-green-fg\">if</span> chunksize <span class=\"ansi-green-fg\">or</span> iterator<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/pandas/io/parsers.py</span> in <span class=\"ansi-cyan-fg\">__init__</span><span class=\"ansi-blue-fg\">(self, f, engine, **kwds)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    893</span>             self<span class=\"ansi-blue-fg\">.</span>options<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#39;has_index_names&#39;</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">=</span> kwds<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#39;has_index_names&#39;</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    894</span> \n<span class=\"ansi-green-fg\">--&gt; 895</span><span class=\"ansi-red-fg\">         </span>self<span class=\"ansi-blue-fg\">.</span>_make_engine<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>engine<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    896</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    897</span>     <span class=\"ansi-green-fg\">def</span> close<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/pandas/io/parsers.py</span> in <span class=\"ansi-cyan-fg\">_make_engine</span><span class=\"ansi-blue-fg\">(self, engine)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1120</span>     <span class=\"ansi-green-fg\">def</span> _make_engine<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> engine<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#39;c&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1121</span>         <span class=\"ansi-green-fg\">if</span> engine <span class=\"ansi-blue-fg\">==</span> <span class=\"ansi-blue-fg\">&#39;c&#39;</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">-&gt; 1122</span><span class=\"ansi-red-fg\">             </span>self<span class=\"ansi-blue-fg\">.</span>_engine <span class=\"ansi-blue-fg\">=</span> CParserWrapper<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>f<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>self<span class=\"ansi-blue-fg\">.</span>options<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1123</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1124</span>             <span class=\"ansi-green-fg\">if</span> engine <span class=\"ansi-blue-fg\">==</span> <span class=\"ansi-blue-fg\">&#39;python&#39;</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/pandas/io/parsers.py</span> in <span class=\"ansi-cyan-fg\">__init__</span><span class=\"ansi-blue-fg\">(self, src, **kwds)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1851</span>         kwds<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#39;usecols&#39;</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>usecols\n<span class=\"ansi-green-intense-fg ansi-bold\">   1852</span> \n<span class=\"ansi-green-fg\">-&gt; 1853</span><span class=\"ansi-red-fg\">         </span>self<span class=\"ansi-blue-fg\">.</span>_reader <span class=\"ansi-blue-fg\">=</span> parsers<span class=\"ansi-blue-fg\">.</span>TextReader<span class=\"ansi-blue-fg\">(</span>src<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kwds<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1854</span>         self<span class=\"ansi-blue-fg\">.</span>unnamed_cols <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_reader<span class=\"ansi-blue-fg\">.</span>unnamed_cols\n<span class=\"ansi-green-intense-fg ansi-bold\">   1855</span> \n\n<span class=\"ansi-green-fg\">pandas/_libs/parsers.pyx</span> in <span class=\"ansi-cyan-fg\">pandas._libs.parsers.TextReader.__cinit__</span><span class=\"ansi-blue-fg\">()</span>\n\n<span class=\"ansi-green-fg\">pandas/_libs/parsers.pyx</span> in <span class=\"ansi-cyan-fg\">pandas._libs.parsers.TextReader._setup_parser_source</span><span class=\"ansi-blue-fg\">()</span>\n\n<span class=\"ansi-red-fg\">FileNotFoundError</span>: [Errno 2] File b&#39;data/train_custom.csv&#39; does not exist: b&#39;data/train_custom.csv&#39;</div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["Let's have a quick look on our data"],"metadata":{}},{"cell_type":"code","source":["data.head(10)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["## 2- Some data exploration"],"metadata":{}},{"cell_type":"markdown","source":["The next step is to gather some information about different column in the DataFrame. You can do so by using **.info()**, which basically gives you information about the number of rows, columns, column data types and memory usage."],"metadata":{}},{"cell_type":"code","source":["print(data.info())"],"metadata":{"scrolled":false},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["As you can see, Dtype column indicates the type of the variable as it is read by pandas.\nColumns like GrLivArea, YearBuilt and TotalBsmtSF are read as **integers**, others like MSZoning, Heating are **object**.\n<br>\nThe columns with object dtype are the **possible** categorical features in the dataset because they are most likely to contain strings.\n<br>\nThe reason why you would say that these categorical features are **'possible'** is because you shouldn't completely rely on **.info()** to get the real data type of the values of a feature, as some missing values that are represented as strings in a continuous feature can coerce it to read them as object dtypes. And also because some categorical features can be encoded as integers.<br><br>\nWe can see for example that the feature TotalBmtSF is recognized as an \"object\", we can think of it as a catogorical feature, but we know that TotalBmtSF is the Total square feet of basement area, so it should be a continious feature. And the reason why it figure out as object type is because maybe it contains some missing values. <br>We can also see that all features have 1459 non-null count, except HeatingQC that contains 80 null entries. \n<br>Missing values will be dealt later on this notebook."],"metadata":{}},{"cell_type":"markdown","source":["### 2.1 - Let's take a look at the distribution of our target SalePrice"],"metadata":{}},{"cell_type":"code","source":["data['SalePrice'].describe()"],"metadata":{"scrolled":true},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["<img src=\"images/saleprice_describe.png\" style=\"width:80%;height:80%;\">"],"metadata":{}},{"cell_type":"code","source":["displayHTML('''<img src=\"/files/tables/saleprice_describe.png\" style=\"width:80%;height:100;\">''')aa"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["sns.distplot(data['SalePrice'], hist=True, kde=False)\n# kde : kernel density estimation, if True allows to plot a gaussian kernel density estimate."],"metadata":{"scrolled":true},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["### 2.2- Let's explore features/variables"],"metadata":{}},{"cell_type":"markdown","source":["As you know, there are two types of features :<br>\n- Continious (or quantitative)\n- Catogorical (or qualitative)\n\n**Continious data** is data which can take an infinite number of possible values; between two different values we can have a new value.<br> Attributes such as cost, price, quantity are typically continious (integers or floats).<br><br>\n**Categorical data** is data which takes on a finite number of possible values. \n<br>2 types of catergorical features : Ordinal (with order) and nominal. For example, if we are talking about a physical product like a t-shirt, it could have categorical variables such as:\n- Size (X-Small, Small, Medium, Large, X-Large), where XS < S < M <L ==> Ordinal type\n- Color (Red, Black, White), no order between theses types ==> nominal type\n\n**The key take away** is that whether or not a variable is categorical depends on its application. Since we only have 3 colors of shirts, then that is a good categorical variable. However, “color” could represent thousands of values in other situations so it would not be a good choice.<br> *So there is no hard and fast rule for how many values a categorical value should have. You should apply your domain knowledge to make that determination on your own data sets.*"],"metadata":{}},{"cell_type":"markdown","source":["#### In the house price dataset, there are the two types of features\nWe based on the panel values of these features in the dataset, and also on some domain knowledge to decide whether a feature is continious or categorical:"],"metadata":{}},{"cell_type":"code","source":["displayHTML('''<img src=\"/files/images_/feature_type.png\" style=\"width:350;height:200;\">''')"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["### 2.2.1- First, let's focus on continious features"],"metadata":{}},{"cell_type":"markdown","source":["**GrLivArea** is the above ground living area in square feet : we can see in the first graph below the distribution of GrLivArea and in the second graph the relationship between the GrLivArea and SalePrice.\n<br>Notice that houses tend to be more expensive as the area increase, and the relationship between the area and the price seems to be linear."],"metadata":{}},{"cell_type":"code","source":["sns.distplot(data['GrLivArea'], hist=True, kde=False)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["#scatter plot grlivarea/saleprice\n#GrLivArea: Above grade (ground) living area square feet.\nvar = 'GrLivArea'\ndata_ = pd.concat([data['SalePrice'], data[var]], axis=1)\ndata_.plot.scatter(x=var, y='SalePrice', ylim=(0,800000))"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["As seen before, **TotalBmstSF** values has object type while it should have float type. <br>\nWe try in the below cell to convert the type of the column, but got an **error**. This confirms that there are some unexpected strings in the data (You can uncomment the cell to see the error if you want):"],"metadata":{}},{"cell_type":"code","source":["#data['TotalBsmtSF'].astype('float')"],"metadata":{"scrolled":true},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["By displaying the content of TotalBsmtSF column, and looking at some values, we can see that the first value is a string \"whitespace\":"],"metadata":{}},{"cell_type":"code","source":["set(data['TotalBsmtSF'])"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["How many times does the whitespace string occur in TotalBsmtSF columns ?"],"metadata":{}},{"cell_type":"code","source":["print(\"There are \", len(data[data[\"TotalBsmtSF\"]==' ']) , \" entries having whitespace in their TotalBsmtSF\")"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["These 50 entries are missing values, we can choose to delete them from our dataset (delete rows or the column). But if at each time there are some mising values in some column, we choose to delete the row or column, we may end up with only few rows and columns in our dataset. \n<br> There are multiple ways to deal with missing values. \n<br>Before choosing which strategy is the most appropriate, we should try to find out if a missing value has a specific meaning depending on the context. \n<br> In the context of TotalBsmtSF, this column contain the area of the basement if there is a basement. If there is no basement in the house, this value is put on 0. \n<br> So maybe the most appropriate way to deal with missing values TotalBsmtSF, is to consider these houses as house without a basement, and replace the whitespace values by zero. \n<br><br> \nThere are many techniques to fill missing values (using the mean or the k-nearest neighbours for example). It depends on the context and the domain knowledge. \n<br>*note: if you have to deal with missing values when working with client data, you should validate the filling strategy with your client*<br><br>\nHere is a link if you want to learn more about working with missing values: https://towardsdatascience.com/how-to-handle-missing-data-8646b18db0d4\n<br>\n<br>\nSo we replace the whitespace string by 0, and we can now convert the column type to float:"],"metadata":{}},{"cell_type":"code","source":["data['TotalBsmtSF'] = data['TotalBsmtSF'].replace(' ', 0)\ndata['TotalBsmtSF'] = data['TotalBsmtSF'].astype('float')"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["The distribution of the TotalBsmtSF looks pretty similar to the GrLivArea one"],"metadata":{}},{"cell_type":"code","source":["#scatter plot TotalBsmtSF/saleprice\n#TotalBsmtSF: Total square feet of basement area.\nvar = 'TotalBsmtSF'\ndata_ = pd.concat([data['SalePrice'], data[var]], axis=1)\ndata_.plot.scatter(x=var, y='SalePrice', ylim=(0,800000))"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["let's look if there are some dependencies between the SalePrice, and the Year of built, the number of bathroom or bedroom"],"metadata":{}},{"cell_type":"code","source":["#scatter plot YearBuilt/saleprice\n#YearBuilt: Original construction date.\nvar = 'YearBuilt'\ndata_ = pd.concat([data['SalePrice'], data[var]], axis=1)\ndata_.plot.scatter(x=var, y='SalePrice', ylim=(0,800000))"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["#scatter plot FullBath/saleprice\n#FullBath: Full bathrooms above grade.   \nvar = 'FullBath'\ndata_ = pd.concat([data['SalePrice'], data[var]], axis=1)\ndata_.plot.scatter(x=var, y='SalePrice', ylim=(0,800000))"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["#scatter plot BedroomAbvGr/saleprice\n#BedroomAbvGr:  Bedrooms above grade (does NOT include basement bedrooms)\nvar = 'BedroomAbvGr'\ndata_ = pd.concat([data['SalePrice'], data[var]], axis=1)\ndata_.plot.scatter(x=var, y='SalePrice', ylim=(0,800000))"],"metadata":{"scrolled":true},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["### What about correlation between continious features ?"],"metadata":{}},{"cell_type":"markdown","source":["The most basic idea of correlation is \"as one variable increases, does the other variable increase (positive correlation), decrease (negative correlation), or stay the same (no correlation)\" with a scale such that perfect positive correlation is +1, no correlation is 0, and perfect negative correlation is -1. <br>\nJust like the month-of-the-year is correlated with the average daily temperature, and the hour-of-the-day is correlated with the amount of light outdoors. <br>\nThe most commonly used mathematical defintion of correlation between continious features is **Pearson's R** which result in a range of [-1,1]\n<img src=\"images/pearson_correlation.png\" style=\"width:400px;height:90px;\">\n<br>Pearson's R works well for calculation correlation between two continious features, we'll se later an alternative for the correlation between categorical features"],"metadata":{}},{"cell_type":"code","source":["cont_features= [\"GrLivArea\",\"TotalBsmtSF\",\"YearBuilt\",\"FullBath\",\"BedroomAbvGr\",\"KitchenAbvGr\",\"GarageCars\",\"PoolArea\"]\ndata[cont_features].head()"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["corr = data[cont_features].corr()#by default pearson\nsns.heatmap(corr, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values)"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["### Build our model using continious features only"],"metadata":{}},{"cell_type":"markdown","source":["Before introducing Categorical features, why not trying to fit a linear model only on continious features and see if we can get good prediction only based them?<br>\nUsing the 8 continious features : GrLivArea , TotalBsmtSF, YearBuilt, FullBath, BedroomAbvGr, KitchenAbvGr, GarageCars, PoolArea \n\n- First, we are going to split our dataset in train set (using to train the model) and test set(using to evaluate the model).\n- Then we are going to re-scale the data, in order to have the same scale of values on all features.(cf. Feature scaling below)\n- Then, we train a linear regression model based on training set to set parameters of the model((theta1, theta2, ..., thetaN) on their most optimal values. The training of the model will occur through multiple iterations, where each iteration aims to update the parameters in order to reduce a certain error. In our example, we use the LinearRegression function from Scikit-learn package, which uses the Mean squared error **(mse)** as an error calculation.\n- Finally, we'll test the trained model on test set and evaluate the results.(we'll also evaluate results on training set, to try to find if there is some overfitting (when model performs very well on training data and very bad on testing data)).\n<br><br><br>\n<img src='files/images_/linear_reg.jpg' style=\"width:10%;height:10%;\">\n<br>\n<img src='files/images_/h.PNG' style=\"width:30%;height:30%;\">\n<br>\n<br>\n\n**Feature scaling**\n<br>Features do usually not have the same range of values. In our example, the feature GrLivArea has values between [334,5642] while FullBath feature range from 0 to 3, and sometimes the gap can be much greater for example when one of the features represents a price where we find much more zeros. (100000 ..)\n<br>The difference in scale may not be liked by the algorithme, where he will spend more time to find his optimal solution (especially when using gradient descent!).\n<br>So feature scaling consists on putting all the features and even the target at the same scale before training the model. \n\n<img src='files/images_/scaling.png'>\n\nOne of the methods to scale data is to use the Standard scaler which perform this transformation on each feature:\n<img src='files/images_/standardscaler.jpg'>"],"metadata":{}},{"cell_type":"code","source":["#Get data with continious features only\nX_continious = data[cont_features]\ny=data['SalePrice']\n\n#Split data on training and testing set\nX_train, X_test, y_train, y_test = train_test_split(X_continious, y, test_size=0.33, random_state=42)\n\n#Scale the data using the standard scaler\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\n\n#train data\nreg = LinearRegression().fit(X_train_scaled, y_train)\n\n#evaluate on training data\ny_pred_train = reg.predict(X_train_scaled)\nprint(\"Mean squared error on Training data : \", np.sqrt(mse(y_pred_train, y_train)))\n"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["y_pred_test = reg.predict(scaler.transform(X_test))\nprint(\"Mean squared error on Training data : \", np.sqrt(mse(y_pred_test, y_test)))"],"metadata":{"scrolled":true},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":["Model perfoms better on training set which is not a surprise while it was trained to fit the training data. \n<br>The MSE for test data is not so bad comparing to training data even if there is some overfitting. \n<br>If we compare the prediction using the model compared to a lazy prediction which will simply take the mean of the salePrice, we can see that even with a limited number of features we can get model results 2 times better."],"metadata":{}},{"cell_type":"code","source":["print(\"mse with basic mean : \", np.sqrt(mse(pd.Series((np.full(len(y_test), y_test.mean()))), y_test)))"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["### 2.2.2- Categorical features"],"metadata":{}},{"cell_type":"markdown","source":["Previously, we put the focus on continious features only. We explored values of continious data and build a model based on 8 continious features. Now, let's consider the categorical features and add them to our model. \n<br> First, same as we did for continous features, we'll de some data exploration on categorical values\n<br> Then, we'll add categorical feature to our model, and see if it improve the prediction."],"metadata":{}},{"cell_type":"markdown","source":["Boxplot is one of the likely useful vizualisation of categorical values\n<img src='images/boxplot.png' style=\"width:60%;height:60%;\">\n\n**Extremely low values** are considered as outliers, in that case the minimum of the box plot is set on Q1-1.5*IQR*<br>\n**Extremely hign values** are considerd as outliers, in that case the maximum of the box plot is set on Q3+1.5*IQR*"],"metadata":{}},{"cell_type":"markdown","source":["**MSZoning** identifies the general zoning classification of the sale. \n- A\t    :Agriculture\n- C(all):Commercial\n- FV\t:Floating Village Residential\n- I\t    :Industrial\n- RH\t:Residential High Density\n- RL\t:Residential Low Density\n- RP\t:Residential Low Density Park \n- RM\t:Residential Medium Density"],"metadata":{}},{"cell_type":"code","source":["mszoning_count = data['MSZoning'].value_counts()\nsns.set(style=\"darkgrid\")\nsns.barplot(mszoning_count.index, mszoning_count.values, alpha=0.9)\nplt.title('Frequency Distribution of MSZoning')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('MSZoning', fontsize=12)\nplt.show()\nsns.boxplot(x='MSZoning', y='SalePrice', data=data, order=[\"C(all)\",'RM', \"RH\",\"RL\",\"FV\"])"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":["**Utilities**: Type of utilities available\n- AllPub\t: All public Utilities (E,G,W,& S)\t\n- NoSewr\t: Electricity, Gas, and Water (Septic Tank)\n- NoSeWa\t: Electricity and Gas Only\n- ELO\t: Electricity only"],"metadata":{}},{"cell_type":"code","source":["utilities_count = data['Utilities'].value_counts()\nsns.set(style=\"darkgrid\")\nsns.barplot(utilities_count.index, utilities_count.values, alpha=0.9)\nplt.title('Frequency Distribution of Utilities')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Utilities', fontsize=12)\nplt.show()\ndata['Utilities'].value_counts()"],"metadata":{"scrolled":true},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":["**HeatingQC**: Heating quality and condition\n- Ex\t: Excellent\n- Gd\t: Good\n- TA\t: Average/Typical\n- Fa\t: Fair\n- Po\t: Poor"],"metadata":{}},{"cell_type":"markdown","source":["We saw at the begining of the notebook, that HeatingQC has 80 null values. In this case we choose to fill missing values using the most occured values which is Ex (Excellent)"],"metadata":{}},{"cell_type":"code","source":["data['HeatingQC'].value_counts()"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["data['HeatingQC'] = data['HeatingQC'].fillna(\"Ex\")"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["data['HeatingQC'].value_counts()"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["heating_count = data['HeatingQC'].value_counts()\nsns.set(style=\"darkgrid\")\nsns.barplot(heating_count.index, heating_count.values, alpha=0.9)\nplt.title('Frequency Distribution of Heating')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Heating', fontsize=12)\nplt.show()"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["centralair_count = data['CentralAir'].value_counts()\nsns.set(style=\"darkgrid\")\nsns.barplot(centralair_count.index, centralair_count.values, alpha=0.9)\nplt.title('Frequency Distribution of Central air')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Central air', fontsize=12)\nplt.show()"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["heatingqc_count = data['HeatingQC'].value_counts()\nsns.set(style=\"darkgrid\")\nsns.barplot(heatingqc_count.index, heatingqc_count.values, alpha=0.9)\nplt.title('Frequency Distribution of HeatingQC')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('HeatingQC', fontsize=12)\nplt.show()"],"metadata":{"scrolled":true},"outputs":[],"execution_count":63},{"cell_type":"code","source":["OverallQual_count = data['OverallQual'].value_counts()\nsns.set(style=\"darkgrid\")\nsns.barplot(OverallQual_count.index, OverallQual_count.values, alpha=0.9)\nplt.title('Frequency Distribution of OverallQual')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('OverallQual', fontsize=12)\nplt.show()\nsns.boxplot(x='OverallQual', y='SalePrice', data=data)"],"metadata":{"scrolled":false},"outputs":[],"execution_count":64},{"cell_type":"markdown","source":["# Correlation\nAs seen before the Pearson's R correlation is a good indicator for continious features while for categorical features we do not have integer values to do the calculation.\n<br>To solve this issue, we can imagine converting categorical feature into intergers and then use Pearson's R correlation expression. And one of the method to convert categorical values into integers is to use the **one-hot encoding** transformation function (or also called dummies). The one-hot encoding transformation, will transform a categorical feature size which has for example 3 possibile values (S,M,L) into 3 binary features (Size_S, Size_M, Size_L).\n<br>Using one-hot encoding technique result usually on a number of features pretty much bigger than the initial one, while each value correspond to a column. \n<br> One-hot encoding is very useful and we'll use later on this notebook, but use it to observe correlation make the observation impossible because of the number of features. \n<br>So an alernative to Pearson's R for categorical features is **Cramer’s V** . \n<br>Cramers'V is based on Pearson's chi-squared statistic and use this expression (if you want to know more about it https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_V)\n<img src=\"images/cramersv.png\" style=\"width:400px;height:90px;\">\n\n*If you want to know more about correlation in general*<br>\nhttps://en.wikipedia.org/wiki/Pearson_correlation_coefficient<br>\nhttps://stats.stackexchange.com/questions/119835/correlation-between-a-nominal-iv-and-a-continuous-dv-variable/124618#124618<br>\nhttps://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9"],"metadata":{}},{"cell_type":"code","source":["def cramers_v(x, y):\n    confusion_matrix = pd.crosstab(x,y)\n    chi2 = ss.chi2_contingency(confusion_matrix)[0]#Chi-square test of independence of variables in a contingency table.\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2/n\n    r,k = confusion_matrix.shape\n    return np.sqrt(phi2/min((k-1),(r-1)))\n\ndef correlation_matrix(cols, data):\n    corrM = np.zeros((len(cols),len(cols)))\n    for col1, col2 in itertools.combinations(cols, 2):\n        idx1, idx2 = cols.index(col1), cols.index(col2)\n        corrM[idx1, idx2] = cramers_v(data[col1], data[col2])\n        corrM[idx2, idx1] = corrM[idx1, idx2]\n    for i in range(len(cols)):\n        corrM[i,i]=1\n    corr = pd.DataFrame(corrM, index=cols, columns=cols)\n    return corr\n\ncols = [\"MSZoning\", \"Heating\",\"HeatingQC\",\"CentralAir\",\"OverallQual\"]\ncorr= correlation_matrix(cols, data)\nfig, ax = plt.subplots(figsize=(7, 6))\nax = sns.heatmap(corr, annot=True, ax=ax); ax.set_title(\"Cramer V Correlation between Variables\");"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"markdown","source":["## 3- Build our model using all features"],"metadata":{}},{"cell_type":"markdown","source":["- First convert Categorical on dummies (one-hot encoding)\n- Merge continious features with dummies\n- Split dataset on train and test set\n- Do feature scaling\n- Train model\n- Tvaluate model\n- Regularize model"],"metadata":{}},{"cell_type":"code","source":["data_dummies = data[cols]\nfor cat_col in cols :\n    if cat_col != \"SalePrice\":\n        col_one_hot = pd.get_dummies(data[cat_col], prefix=cat_col)\n        data_dummies = data_dummies.drop(cat_col, axis=1)\n        data_dummies = data_dummies.join(col_one_hot)"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"code","source":["data_dummies.head()"],"metadata":{"scrolled":true},"outputs":[],"execution_count":70},{"cell_type":"code","source":["X = data_dummies.join(data[cont_features])"],"metadata":{"scrolled":true},"outputs":[],"execution_count":71},{"cell_type":"code","source":["y = data['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nscaler_1 = StandardScaler()\nscaler_1.fit(X_train)\nX_train_scaled = scaler_1.transform(X_train)\n\nregression = LinearRegression().fit(X_train, y_train)\n\ny_pred_train = regression.predict(X_train)\nprint(\"mse sklearn : \", np.sqrt(mse(y_pred_train, y_train)))"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"code","source":["y_pred_test = regression.predict(scaler_1.transform(X_test))"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"code","source":["print(\"mse sklearn : \", np.sqrt(mse(y_pred_test, y_test)))"],"metadata":{},"outputs":[],"execution_count":74},{"cell_type":"markdown","source":["### TBD - Explication Ridge regression"],"metadata":{}},{"cell_type":"code","source":["## train model using ridge regression using all variables\nridgeReg = Ridge(alpha=0.5, normalize=True) #Ridge(alpha=0.05, normalize=True)\nridgeReg.fit(X_train_scaled,y_train)\ny_pred_ridge_ = ridgeReg.predict(X_train_scaled)\nprint(\"mse sklearn : \", np.sqrt(mse(y_pred_ridge_, y_train)))"],"metadata":{},"outputs":[],"execution_count":76},{"cell_type":"code","source":["y_test_pred_ridge = ridgeReg.predict(scaler_1.transform(X_test))"],"metadata":{},"outputs":[],"execution_count":77},{"cell_type":"code","source":["print(\"mse sklearn : \", np.sqrt(mse(y_test_pred_ridge, y_test)))"],"metadata":{"scrolled":true},"outputs":[],"execution_count":78},{"cell_type":"markdown","source":["### TBD- explication GridSearch"],"metadata":{}},{"cell_type":"code","source":["model = Ridge()\nparams = {'alpha': np.logspace(-3,3,15)}\n    \ngrid = GridSearchCV(estimator=model, cv=5, param_grid=params, return_train_score=False,\n                        n_jobs=4, refit=True)\n    \ngrid.fit(X_train_scaled, y_train)\ny_pred_ridge_ = grid.predict(X_train_scaled)"],"metadata":{},"outputs":[],"execution_count":80},{"cell_type":"code","source":["print(\"mse sklearn : \", np.sqrt(mse(y_pred_ridge_, y_train)))"],"metadata":{},"outputs":[],"execution_count":81},{"cell_type":"code","source":["y_test_pred_ridge = grid.predict(scaler_1.transform(X_test))\nprint(\"mse sklearn : \", np.sqrt(mse(y_test_pred_ridge, y_test)))"],"metadata":{},"outputs":[],"execution_count":82}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.7.3","nbconvert_exporter":"python","file_extension":".py"},"name":"House prediction tutorial","notebookId":1892949054938731},"nbformat":4,"nbformat_minor":0}
